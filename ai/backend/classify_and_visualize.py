import argparse
import json
from collections import Counter, defaultdict
import math
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import torch
from torch.nn.functional import softmax
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from tqdm import tqdm
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE


LABEL_MAP = {0: "left", 1: "center", 2: "right"}


def read_jsonl(path: Path) -> Iterable[dict]:
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                yield json.loads(line)
            except json.JSONDecodeError:
                continue


def flatten_records(records: Iterable[dict]) -> List[dict]:
    flat: List[dict] = []
    for rec in records:
        row_id = rec.get("row")
        for key in ("q1", "q2", "q3"):
            q = rec.get(key) or {}
            text = (q.get("answer") or "").strip()
            if not text:
                continue
            flat.append(
                {
                    "row": row_id,
                    "slot": key,
                    "question": q.get("question", ""),
                    "position": q.get("position"),
                    "ideology": q.get("ideology"),
                    "text": text,
                }
            )
    return flat


def batch(iterable: List, size: int) -> Iterable[List]:
    for i in range(0, len(iterable), size):
        yield iterable[i : i + size]


def classify_texts(
    texts: List[str],
    tokenizer: AutoTokenizer,
    model: AutoModelForSequenceClassification,
    max_length: int,
) -> Tuple[List[int], List[List[float]], torch.Tensor, torch.Tensor]:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    preds: List[int] = []
    probs_list: List[List[float]] = []
    with torch.no_grad():
        tokens = tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length,
        )
        tokens = {k: v.to(device) for k, v in tokens.items()}
        outputs = model(**tokens, output_hidden_states=True, return_dict=True)
        logits = outputs.logits
        probabilities = softmax(logits, dim=1)
        top_indices = torch.argmax(probabilities, dim=1).tolist()
        preds.extend(top_indices)
        probs_list.extend(probabilities.detach().cpu().tolist())
        last_hidden = outputs.hidden_states[-1]  # [B, T, H]
        cls_embeddings = last_hidden[:, 0, :].detach()  # [B, H]
    return preds, probs_list, cls_embeddings.cpu(), logits.cpu()


def main() -> None:
    parser = argparse.ArgumentParser(description="Classify political leaning and visualize results")
    parser.add_argument(
        "--input",
        type=str,
        default="backend/data/political_qa.jsonl",
        help="Input JSONL generated by the QA script",
    )
    parser.add_argument(
        "--output-jsonl",
        type=str,
        default="backend/data/political_qa_classified.jsonl",
        help="Output JSONL with classifications",
    )
    parser.add_argument(
        "--summary-csv",
        type=str,
        default="backend/data/political_qa_summary.csv",
        help="Summary CSV with counts and percentages",
    )
    parser.add_argument(
        "--fig-dir",
        type=str,
        default="backend/figs",
        help="Directory to save figures",
    )
    parser.add_argument("--batch-size", type=int, default=64, help="Classification batch size")
    parser.add_argument("--max-length", type=int, default=256, help="Tokenizer max length")
    args = parser.parse_args()

    in_path = Path(args.input)
    out_jsonl = Path(args.output_jsonl)
    summary_csv = Path(args.summary_csv)
    fig_dir = Path(args.fig_dir)
    fig_dir.mkdir(parents=True, exist_ok=True)
    out_jsonl.parent.mkdir(parents=True, exist_ok=True)
    summary_csv.parent.mkdir(parents=True, exist_ok=True)

    # Load data
    records = list(read_jsonl(in_path))
    flat = flatten_records(records)
    if not flat:
        print("No records to classify.")
        return

    # Load model and tokenizer as specified in the model card
    tokenizer = AutoTokenizer.from_pretrained("launch/POLITICS")
    model = AutoModelForSequenceClassification.from_pretrained("matous-volf/political-leaning-politics")

    # Classify in batches
    classifications: List[dict] = []
    all_embeddings = []  # for PCA visualization only

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    # Prepare ideology axis from classifier weights (0=left, 2=right)
    W = model.classifier.out_proj.weight.data.to(device)  # [3, H]
    b = model.classifier.out_proj.bias.data.to(device)    # [3]
    a_vec = (W[2] - W[0]).detach().cpu()  # [H]
    c_bias = (b[2] - b[0]).detach().cpu() # scalar

    for chunk in tqdm(list(batch(flat, args.batch_size)), desc="Classifying"):
        texts = [item["text"] for item in chunk]
        (
            pred_indices,
            prob_lists,
            cls_emb_batch,
            logits_batch,
        ) = classify_texts(texts, tokenizer, model, args.max_length)

        # ideology raw score s = z·a + c (higher => more right)
        ideology_raw = (cls_emb_batch @ a_vec) + c_bias  # [B]

        for i, (item, idx, probs) in enumerate(zip(chunk, pred_indices, prob_lists)):
            label = LABEL_MAP.get(idx, str(idx))
            score = float(probs[idx])
            # uncertainty metrics
            sorted_probs = sorted(probs, reverse=True)
            margin = float(sorted_probs[0] - sorted_probs[1])
            # normalized entropy in [0,1]
            entropy = -sum(p * math.log(max(p, 1e-12)) for p in probs) / math.log(3.0)
            extremeness = float(max(probs[0], probs[2]) - probs[1])
            classifications.append(
                {
                    **item,
                    "pred_index": int(idx),
                    "pred_label": label,
                    "pred_score": score,
                    "prob_left": float(probs[0]),
                    "prob_center": float(probs[1]),
                    "prob_right": float(probs[2]),
                    "margin": margin,
                    "entropy": float(entropy),
                    "extremeness": extremeness,
                    "ideology_raw": float(ideology_raw[i].item()),
                }
            )
        all_embeddings.append(cls_emb_batch.numpy())

    # Write per-answer classifications JSONL
    with out_jsonl.open("w", encoding="utf-8") as f:
        for row in classifications:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    # Build summary dataframe
    df = pd.DataFrame(classifications)

    # Continuous ideology score in [-1, 1] via z-scored tanh
    s = df["ideology_raw"].values
    s_mean = s.mean()
    s_std = s.std() if s.std() > 1e-6 else 1.0
    df["ideology_z"] = (df["ideology_raw"] - s_mean) / s_std
    df["ideology_score"] = (df["ideology_z"] / 2.0).apply(lambda x: float(math.tanh(x)))

    # Summary: distribution per question
    summary = (
        df.groupby(["slot", "pred_label"])\
          .size()\
          .reset_index(name="count")
    )
    total_per_slot = summary.groupby("slot")["count"].transform("sum")
    summary["percent"] = (summary["count"] / total_per_slot * 100.0).round(2)
    summary.to_csv(summary_csv, index=False)

    # Detect if ideologies are present
    has_ideology = df["ideology"].notna().any()

    # Figure 1: Distribution by question (stacked bar)
    pivot = summary.pivot(index="slot", columns="pred_label", values="count").fillna(0)
    pivot = pivot[[c for c in ["left", "center", "right"] if c in pivot.columns]]
    sns.set_theme(style="whitegrid")
    ax = pivot.plot(kind="bar", stacked=True, figsize=(9, 5), colormap="coolwarm")
    ax.set_title("Predicted leaning by question")
    ax.set_xlabel("Question slot")
    ax.set_ylabel("Count")
    ax.legend(title="Leaning")
    plt.tight_layout()
    plt.savefig(fig_dir / "leaning_by_question.png", dpi=150)
    plt.close()

    if has_ideology:
        # Figure 2A: Stacked distribution by ideology label (no 1–10)
        ideol_summary = (
            df.groupby(["ideology", "pred_label"]).size().reset_index(name="count")
        )
        ideol_pivot = ideol_summary.pivot(index="ideology", columns="pred_label", values="count").fillna(0)
        ideol_pivot = ideol_pivot[[c for c in ["left", "center", "right"] if c in ideol_pivot.columns]]
        ideol_pivot = ideol_pivot.sort_values(by=ideol_pivot.columns.tolist(), ascending=False)
        ax2a = ideol_pivot.plot(kind="bar", stacked=True, figsize=(12, 6), colormap="coolwarm")
        ax2a.set_title("Predicted leaning by ideology label")
        ax2a.set_xlabel("Ideology")
        ax2a.set_ylabel("Count")
        ax2a.legend(title="Leaning")
        plt.tight_layout()
        plt.savefig(fig_dir / "leaning_by_ideology.png", dpi=150)
        plt.close()
    else:
        # Figure 2: Stacked distribution by assigned position (1..10)
        pos_summary = (
            df.groupby(["position", "pred_label"]).size().reset_index(name="count")
        )
        pos_pivot = pos_summary.pivot(index="position", columns="pred_label", values="count").fillna(0)
        pos_pivot = pos_pivot[[c for c in ["left", "center", "right"] if c in pos_pivot.columns]]
        ax2 = pos_pivot.plot(kind="bar", stacked=True, figsize=(10, 5), colormap="coolwarm")
        ax2.set_title("Predicted leaning by assigned political position")
        ax2.set_xlabel("Assigned position (1=far left, 10=far right)")
        ax2.set_ylabel("Count")
        ax2.legend(title="Leaning")
        plt.tight_layout()
        plt.savefig(fig_dir / "leaning_by_assigned_position.png", dpi=150)
        plt.close()

    # Figure 3: If ideologies exist, skip 1–10 violin; otherwise show by position
    if not has_ideology:
        plt.figure(figsize=(10, 5))
        sns.violinplot(data=df, x="position", y="ideology_score", inner="quartile", palette="coolwarm")
        plt.title("Continuous ideology score by assigned position")
        plt.xlabel("Assigned position (1=far left, 10=far right)")
        plt.ylabel("Ideology score (-1 left … +1 right)")
        plt.tight_layout()
        plt.savefig(fig_dir / "ideology_by_position_violin.png", dpi=150)
        plt.close()

    # Figure 4: KDE of ideology score by predicted label
    plt.figure(figsize=(9, 5))
    for label, color in zip(["left", "center", "right"], ["#4575b4", "#91bfdb", "#d73027"]):
        subset = df[df["pred_label"] == label]["ideology_score"]
        if len(subset) > 1:
            sns.kdeplot(subset, label=label, fill=True, alpha=0.3, color=color)
    plt.title("Density of ideology scores by predicted label")
    plt.xlabel("Ideology score (-1 left … +1 right)")
    plt.ylabel("Density")
    plt.legend()
    plt.tight_layout()
    plt.savefig(fig_dir / "ideology_kde_by_label.png", dpi=150)
    plt.close()

    # Figure 5+: Embedding visualizations
    try:
        import numpy as np

        X = np.vstack(all_embeddings)
        pca = PCA(n_components=2, random_state=42)
        pc = pca.fit_transform(X)
        df["pc1"] = pc[:, 0]
        df["pc2"] = pc[:, 1]

        # PCA by predicted label
        plt.figure(figsize=(9, 6))
        palette = {"left": "#4575b4", "center": "#91bfdb", "right": "#d73027"}
        sizes = 50 * (df["extremeness"].clip(lower=0) / (df["extremeness"].max() + 1e-6) + 0.2)
        for label in ["left", "center", "right"]:
            sub = df[df["pred_label"] == label]
            plt.scatter(sub["pc1"], sub["pc2"], s=sizes.loc[sub.index], c=palette.get(label, "gray"), label=label, alpha=0.6, edgecolors="none")
        plt.title("PCA of CLS embeddings (size ∝ extremeness)")
        plt.xlabel("PC1")
        plt.ylabel("PC2")
        plt.legend(title="Predicted label")
        plt.tight_layout()
        plt.savefig(fig_dir / "pca_embeddings_by_label.png", dpi=150)
        plt.close()

        # PCA colored by assigned position (only if no ideology labels)
        if not has_ideology:
            plt.figure(figsize=(9, 6))
            sc = plt.scatter(df["pc1"], df["pc2"], c=df["position"], cmap="viridis", alpha=0.7, s=40, edgecolors="none")
            plt.title("PCA of CLS embeddings colored by assigned position")
            plt.xlabel("PC1")
            plt.ylabel("PC2")
            cbar = plt.colorbar(sc)
            cbar.set_label("Assigned position")
            plt.tight_layout()
            plt.savefig(fig_dir / "pca_embeddings_by_position.png", dpi=150)
            plt.close()

        # PCA centroids by ideology (no 1–10)
        if has_ideology:
            plt.figure(figsize=(10, 7))
            plt.scatter(df["pc1"], df["pc2"], c="#bbbbbb", alpha=0.2, s=10, edgecolors="none")
            centroids = df.groupby("ideology")[ ["pc1", "pc2"] ].mean().reset_index()
            counts = df.groupby("ideology").size().rename("count").reset_index()
            centroids = centroids.merge(counts, on="ideology")
            unique_ideol = centroids["ideology"].tolist()
            colors = sns.color_palette(n_colors=len(unique_ideol))
            color_map = {ideol: col for ideol, col in zip(unique_ideol, colors)}
            for _, row in centroids.iterrows():
                plt.scatter(row["pc1"], row["pc2"], s=80 + 3 * row["count"], c=[color_map[row["ideology"]]], edgecolors="black", linewidths=0.5)
                plt.text(row["pc1"] + 0.02, row["pc2"] + 0.02, row["ideology"], fontsize=8)
            plt.title("PCA centroids by ideology (size ∝ count)")
            plt.xlabel("PC1")
            plt.ylabel("PC2")
            plt.tight_layout()
            plt.savefig(fig_dir / "pca_centroids_by_ideology.png", dpi=150)
            plt.close()

        # t-SNE (label)
        try:
            tsne = TSNE(n_components=2, perplexity=30, learning_rate="auto", init="pca", n_iter=1000, random_state=42)
            ts = tsne.fit_transform(X)
            df["tsne1"] = ts[:, 0]
            df["tsne2"] = ts[:, 1]

            plt.figure(figsize=(9, 6))
            for label in ["left", "center", "right"]:
                sub = df[df["pred_label"] == label]
                plt.scatter(sub["tsne1"], sub["tsne2"], s=sizes.loc[sub.index], c=palette.get(label, "gray"), label=label, alpha=0.6, edgecolors="none")
            plt.title("t-SNE of CLS embeddings (size ∝ extremeness)")
            plt.xlabel("t-SNE 1")
            plt.ylabel("t-SNE 2")
            plt.legend(title="Predicted label")
            plt.tight_layout()
            plt.savefig(fig_dir / "tsne_embeddings_by_label.png", dpi=150)
            plt.close()

            # t-SNE (position) only if no ideologies
            if not has_ideology:
                plt.figure(figsize=(9, 6))
                sc = plt.scatter(df["tsne1"], df["tsne2"], c=df["position"], cmap="viridis", alpha=0.7, s=40, edgecolors="none")
                plt.title("t-SNE of CLS embeddings colored by assigned position")
                plt.xlabel("t-SNE 1")
                plt.ylabel("t-SNE 2")
                cbar = plt.colorbar(sc)
                cbar.set_label("Assigned position")
                plt.tight_layout()
                plt.savefig(fig_dir / "tsne_embeddings_by_position.png", dpi=150)
                plt.close()

            # t-SNE centroids by ideology
            if has_ideology:
                plt.figure(figsize=(10, 7))
                plt.scatter(df["tsne1"], df["tsne2"], c="#bbbbbb", alpha=0.2, s=10, edgecolors="none")
                cent_ts = df.groupby("ideology")[ ["tsne1", "tsne2"] ].mean().reset_index()
                counts = df.groupby("ideology").size().rename("count").reset_index()
                cent_ts = cent_ts.merge(counts, on="ideology")
                unique_ideol = cent_ts["ideology"].tolist()
                colors = sns.color_palette(n_colors=len(unique_ideol))
                color_map = {ideol: col for ideol, col in zip(unique_ideol, colors)}
                for _, row in cent_ts.iterrows():
                    plt.scatter(row["tsne1"], row["tsne2"], s=80 + 3 * row["count"], c=[color_map[row["ideology"]]], edgecolors="black", linewidths=0.5)
                    plt.text(row["tsne1"] + 0.5, row["tsne2"] + 0.5, row["ideology"], fontsize=8)
                plt.title("t-SNE centroids by ideology (size ∝ count)")
                plt.xlabel("t-SNE 1")
                plt.ylabel("t-SNE 2")
                plt.tight_layout()
                plt.savefig(fig_dir / "tsne_centroids_by_ideology.png", dpi=150)
                plt.close()
        except Exception:
            pass

        # UMAP (optional)
        try:
            import umap

            reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
            um = reducer.fit_transform(X)
            df["umap1"] = um[:, 0]
            df["umap2"] = um[:, 1]

            plt.figure(figsize=(9, 6))
            for label in ["left", "center", "right"]:
                sub = df[df["pred_label"] == label]
                plt.scatter(sub["umap1"], sub["umap2"], s=sizes.loc[sub.index], c=palette.get(label, "gray"), label=label, alpha=0.6, edgecolors="none")
            plt.title("UMAP of CLS embeddings (size ∝ extremeness)")
            plt.xlabel("UMAP 1")
            plt.ylabel("UMAP 2")
            plt.legend(title="Predicted label")
            plt.tight_layout()
            plt.savefig(fig_dir / "umap_embeddings_by_label.png", dpi=150)
            plt.close()

            if not has_ideology:
                plt.figure(figsize=(9, 6))
                sc = plt.scatter(df["umap1"], df["umap2"], c=df["position"], cmap="viridis", alpha=0.7, s=40, edgecolors="none")
                plt.title("UMAP of CLS embeddings colored by assigned position")
                plt.xlabel("UMAP 1")
                plt.ylabel("UMAP 2")
                cbar = plt.colorbar(sc)
                cbar.set_label("Assigned position")
                plt.tight_layout()
                plt.savefig(fig_dir / "umap_embeddings_by_position.png", dpi=150)
                plt.close()

            if has_ideology:
                plt.figure(figsize=(10, 7))
                plt.scatter(df["umap1"], df["umap2"], c="#bbbbbb", alpha=0.2, s=10, edgecolors="none")
                cent_um = df.groupby("ideology")[ ["umap1", "umap2"] ].mean().reset_index()
                counts = df.groupby("ideology").size().rename("count").reset_index()
                cent_um = cent_um.merge(counts, on="ideology")
                unique_ideol = cent_um["ideology"].tolist()
                colors = sns.color_palette(n_colors=len(unique_ideol))
                color_map = {ideol: col for ideol, col in zip(unique_ideol, colors)}
                for _, row in cent_um.iterrows():
                    plt.scatter(row["umap1"], row["umap2"], s=80 + 3 * row["count"], c=[color_map[row["ideology"]]], edgecolors="black", linewidths=0.5)
                    plt.text(row["umap1"] + 0.02, row["umap2"] + 0.02, row["ideology"], fontsize=8)
                plt.title("UMAP centroids by ideology (size ∝ count)")
                plt.xlabel("UMAP 1")
                plt.ylabel("UMAP 2")
                plt.tight_layout()
                plt.savefig(fig_dir / "umap_centroids_by_ideology.png", dpi=150)
                plt.close()
        except Exception:
            pass

        # PC1 vs ideology score
        plt.figure(figsize=(8, 5))
        sns.regplot(x=df["pc1"], y=df["ideology_score"], scatter_kws={"alpha": 0.4, "s": 30})
        corr = np.corrcoef(df["pc1"], df["ideology_score"])[0, 1]
        plt.title(f"PC1 vs Ideology score (r={corr:.2f})")
        plt.xlabel("PC1")
        plt.ylabel("Ideology score (-1 left … +1 right)")
        plt.tight_layout()
        plt.savefig(fig_dir / "pc1_vs_ideology_score.png", dpi=150)
        plt.close()

    except Exception:
        pass

    print(f"Wrote classifications to {out_jsonl}")
    print(f"Wrote summary to {summary_csv}")
    print(f"Saved figures to {fig_dir}")


if __name__ == "__main__":
    main()